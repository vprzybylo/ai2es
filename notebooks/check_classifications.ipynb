{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "import csv\n",
    "import os\n",
    "import cocpit.config as config\n",
    "import cocpit\n",
    "from ai2es import gui_test_loader\n",
    "import ipywidgets\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import ImagePath\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If checking classifications from realtime predictions in csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2022/08/06'\n",
    "pred_dir = '/ai2es/realtime_predictions/csv'\n",
    "pred = 'precipitation'\n",
    "file_list = []\n",
    "open_dir = []\n",
    "with open(f\"{pred_dir}/{date}/{date.replace('/','_')}.csv\", newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        file_list.append(row[0])\n",
    "        station = row[0].split('.')[0].split('_')[1]\n",
    "        open_dir.append(f'/ai2es/cam_photos/{date}/{station}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if checking classifications from test folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station_id = 'VOOR'\n",
    "#open_dir = f\"/ai2es/test_set/{station_id}\"\n",
    "open_dir = '/ai2es/test_set/shap/'\n",
    "file_list = os.listdir(open_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = cocpit.data_loaders.TestDataSet(open_dir, file_list)\n",
    "test_loader = cocpit.data_loaders.create_loader(test_data, batch_size=100, sampler=None)\n",
    "p = cocpit.predictions.LoaderPredictions()\n",
    "with torch.no_grad():\n",
    "    for fold in range(config.KFOLD+1):\n",
    "        model = p.load_model(fold)\n",
    "        for (imgs, paths) in test_loader:\n",
    "            b = cocpit.predictions.BatchPredictions(imgs, model)\n",
    "            b.find_max_preds()\n",
    "            b.top_k_preds(len(config.CLASS_NAMES))\n",
    "            p.append_batch(b, paths)\n",
    "p.concatenate_loader_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file in the test loader,\n",
    "# display 1 min accumulated precip quantity \n",
    "# siftfing through all of the images to find files in test set\n",
    "df = pd.read_parquet(\"/ai2es/matched_parquet/2017.parquet\")\n",
    "search_paths = df['path'].str.split('/')\n",
    "search_paths = search_paths.apply(lambda x: x[-1])\n",
    "precip = [df['precip_accum_1min [mm]'][search_paths == file] for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515ecca623ab4f7b97b91b9a05389544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Button(description='Next', style=ButtonStyle())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gui = gui_test_loader.GUI(p.paths, p.topk_probs, p.topk_classes, precip=None)\n",
    "gui.interp()\n",
    "display(ipywidgets.HBox([gui.output, gui.next_btn]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a prediction on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[9.5876e-02, 1.3070e-05, 9.0411e-01]], device='cuda:0',\n",
       "        grad_fn=<SoftmaxBackward>),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize\n",
    "import numpy as np\n",
    "from cocpit import config as config\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def preprocess_image(\n",
    "    img: np.ndarray, mean, std) -> torch.Tensor:\n",
    "    preprocessing = Compose([\n",
    "        ToTensor(),\n",
    "        Resize((224,224)),\n",
    "        Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    return preprocessing(img.copy()).unsqueeze(0)\n",
    "\n",
    "\n",
    "model = torch.load(\"/ai2es/saved_models/v0.0.0/e[30]_bs[64]_k0_1model(s).pt\")\n",
    "model.eval()\n",
    "rgb_img = cv2.imread('/ai2es/cam_photos/2017/04/02/CLAR/20170402T002001_CLAR.jpg', 1)[:, :, ::-1]\n",
    "rgb_img = np.float32(rgb_img) / 255\n",
    "input_tensor = preprocess_image(rgb_img,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "outputs=model(input_tensor)\n",
    "preds = F.softmax(outputs, dim=1)\n",
    "_, max_preds = torch.max(outputs, 1)\n",
    "max_preds = max_preds.cpu()\n",
    "preds, max_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
