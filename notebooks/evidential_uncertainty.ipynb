{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidential Deep Learning to Quantify Classification Uncertainty\n",
    "- aimed at detecting out-of-distribution queries and adversarial samples\n",
    "- penalizes incorrect evidence on errors and out-of-distribution examples through loss function\n",
    "    - evidence is a measure of the amount of support\n",
    "collected from data in favor of a sample to be classified into a certain class\n",
    "    - tries to shrink the total evidence (relu activation) to zero if the sample cannot be correctly classified \n",
    "    - uses Kullbackâ€“Leibler divergence (KL) loss to force the model to be uncertain especially when its prediction is incorrect through mean squared error, log likelihood, or digamma\n",
    "- recognizes when the model is likely to fail\n",
    "\n",
    "There are two axes of NN uncertainty that can be modeled: \n",
    "- (1) uncertainty in the data, called aleatoric uncertainty, and \n",
    "- (2) uncertainty in the prediction, called epistemic uncertainty.\n",
    "\n",
    "This is a computationally efficient alternative to model ensembles, Bayesion NNs, and monte carlo dropout, which also gets at model uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cocpit import config as config\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" functions for uncertainty calculations \"\"\"\n",
    "def model_logit_output(img: PIL.Image) -> torch.Tensor:\n",
    "    \"\"\"run model on image and output logit\"\"\"\n",
    "    trans = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "           ),])\n",
    "    img_tensor = trans(img)\n",
    "    img_tensor.unsqueeze_(0)\n",
    "    img_variable = Variable(img_tensor)\n",
    "    return model(img_variable)\n",
    "\n",
    "def calc_uncertainty(model: torch.nn.parallel.DataParallel, output: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"calculate max probability prediction, class probability, and uncertainty\"\"\"\n",
    "    softmax = F.softmax(output, dim=1)\n",
    "    evidence = F.relu(output)\n",
    "    alpha = evidence + 1\n",
    "    # uncertainty calculated as # of classes/total evidence summed across classes + 1\n",
    "    uncertainty = len(config.CLASS_NAMES) / torch.sum(alpha, dim=1, keepdim=True)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    prob = alpha / torch.sum(alpha, dim=1, keepdim=True)\n",
    "    output = output.flatten()\n",
    "    prob = softmax.flatten()\n",
    "    pred = pred.flatten()\n",
    "    return (pred, prob, uncertainty)\n",
    "\n",
    "def uncertainty_examples(model: torch.nn.parallel.DataParallel, img_paths: List[str]) -> None:\n",
    "    \"\"\"loop through a sample of images, make predictions, and output uncertainty\"\"\"\n",
    "    for img_path in img_paths:\n",
    "        img = Image.open(img_path)\n",
    "        output = model_logit_output(img)\n",
    "        pred, prob, uncertainty = calc_uncertainty(model, output)\n",
    "        plot_uncertainty(img, pred, prob, uncertainty)\n",
    "\n",
    "def plot_uncertainty(img: PIL.Image, preds: torch.Tensor, prob: torch.Tensor, uncertainty: torch.Tensor, precip:str=None, fontsize: int=16) -> None:\n",
    "    \"\"\"plot image above with bar chart below of probability and uncertainty value in title\"\"\"\n",
    "    labels = np.arange(len(config.CLASS_NAMES))\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(6, 12), gridspec_kw={\"height_ratios\": [3, 1]})\n",
    "\n",
    "    plt.title(f\"Classified as: {config.CLASS_NAMES[preds[0]]}, Uncertainty: {np.round(uncertainty.item(), 3)}\", fontsize=fontsize)\n",
    "    \n",
    "    axs[0].imshow(img, cmap=\"gray\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].bar(labels, prob.cpu().detach().numpy(), width=0.5)\n",
    "    plt.xticks(np.arange(len(config.CLASS_NAMES)), config.CLASS_NAMES, rotation='vertical', fontsize=fontsize)\n",
    "\n",
    "    axs[1].set_ylim([0, 1])\n",
    "    axs[1].set_xticklabels(config.CLASS_NAMES, fontsize=fontsize)\n",
    "\n",
    "    axs[1].set_xlabel(\"Classes\", fontsize=fontsize+2)\n",
    "    axs[1].set_ylabel(\"Softmax Probability\", fontsize=fontsize+2)\n",
    "    if precip:\n",
    "        ax[1].set_title(f'Accumulated Precip: {precip}')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    #plt.savefig(\"{/home/vanessa/hulk/ai2es}/plots/{}\".format(os.path.basename(img_path)))\n",
    "\n",
    "def uncertainty_acc_hist(grouped_df: pd.DataFrame) -> None:\n",
    "    \"\"\"histogram of accuracy binned by uncertainty\"\"\"\n",
    "    fig, ax = plt.subplots(1,1, figsize=(10,7))\n",
    "    grouped_df['acc'].plot(kind='bar', ax=ax, color='k')\n",
    "    plt.xlabel('Uncertainty Range', fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    xlocs, xlabs = plt.xticks()\n",
    "    plt.ylim(0.0,1.10)\n",
    "    plt.ylabel('Mean Accuracy [%]', fontsize=18)\n",
    "    for index, (c, acc) in enumerate(zip(grouped_df['count'], grouped_df['acc'])):\n",
    "        shift=0.2 if c < 100 else 0.3\n",
    "        plt.text(xlocs[index]-shift, acc+0.02, f'{c}', fontsize=16)\n",
    "    plt.savefig(f'{config.BASE_DIR}/plots/uncertainty_vs_acc_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"test uncertainty calculation on some hand chosen examples in and out of distribution norms\"\"\"\n",
    "# sourcery skip: avoid-global-variables\n",
    "\n",
    "model = torch.load('{/home/vanessa/hulk/ai2es}/saved_models/v0.0.0/e[20]_bs[64]_k0_1model(s)_evidential.pt')\n",
    "img_paths = [f'{config.BASE_DIR}/codebook_dataset/combined_extra/obstructed/20170103T223007_NHUD.jpg',\n",
    "                f'{config.BASE_DIR}/codebook_dataset/combined_extra/no_precip/20170101T080002_ANDE.jpg',\n",
    "                f'{config.BASE_DIR}/codebook_dataset/combined_extra/no_precip/20170101T112501_ANDE.jpg',\n",
    "                f'{config.BASE_DIR}/codebook_dataset/combined_extra/precip/20170101T030002_ANDE.jpg',\n",
    "                f'{config.BASE_DIR}/codebook_dataset/combined_extra/precip/20220225T112038_WEST.jpg',\n",
    "                f'{config.BASE_DIR}/codebook_dataset/combined_extra/precip/20220312T010027_BUFF.jpg',\n",
    "                f'{config.BASE_DIR}/notebooks/Oklahoma-Mesonet.png',\n",
    "                f'{config.BASE_DIR}/notebooks/thunderstorm-and-lightning.png'\n",
    "                ]\n",
    "uncertainty_examples(model, img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for each file in the test loader,\n",
    "display 1 min accumulated precip quantity \n",
    "siftfing through all of the images to find files in test set\n",
    "\"\"\"\n",
    "val_loader = torch.load('{/home/vanessa/hulk/ai2es}/saved_val_loaders/v0.0.0/e[20]_val_loader20_bs[64]_k0_1model(s)_evidential.pt')\n",
    "\n",
    "paths = []\n",
    "for (img, label, path), _ in val_loader:\n",
    "    basedir, filename = os.path.split(path)\n",
    "    paths.append(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"{/home/vanessa/hulk/ai2es}/matched_parquet/2017.parquet\").reset_index()\n",
    "# df_base, df_files = [],[]\n",
    "# for i in df['path']:\n",
    "#     df_b, df_f = os.path.split(i)\n",
    "#     df_base.append(df_b)\n",
    "#     df_files.append(df_f)\n",
    "\n",
    "precip = df['precip_accum_1min [mm]'][pd.Series(df_files).isin(paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.00\n",
       "4          0.00\n",
       "5          0.00\n",
       "147        0.00\n",
       "148        0.00\n",
       "           ... \n",
       "7573366    0.00\n",
       "7575644    0.10\n",
       "7577552    0.00\n",
       "7578731    0.08\n",
       "7578737    0.07\n",
       "Name: precip_accum_1min [mm], Length: 8768, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Find distribution of evidential uncertainties and softmax probabilitie on validation dataloader\"\"\"\n",
    "# sourcery skip: avoid-global-variables\n",
    "\n",
    "model = torch.load('{/home/vanessa/hulk/ai2es}/saved_models/v0.0.0/e[20]_bs[64]_k0_1model(s)_evidential.pt').to(config.DEVICE)\n",
    "val_loader = torch.load('{/home/vanessa/hulk/ai2es}/saved_val_loaders/v0.0.0/e[20]_val_loader20_bs[64]_k0_1model(s)_evidential.pt')\n",
    "correct = 0\n",
    "preds, labels, probs, uncertainties, paths = [], [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for ((img, label, path), _) in val_loader:\n",
    "        img = PIL.Image.open(path)\n",
    "        output = model_logit_output(img)\n",
    "        pred, prob, uncertainty = calc_uncertainty(model, output)\n",
    "        preds.append(pred.item())\n",
    "        labels.append(label)\n",
    "        probs.append(prob.cpu().numpy())\n",
    "        uncertainties.append(uncertainty.item())\n",
    "        paths.append(path)\n",
    "        #correct += (pred[0] == label).float().sum()\n",
    "        #total = i\n",
    "        if uncertainty > 0.4:# and prob[pred[0]] > 0.6:\n",
    "            plot_uncertainty(img, pred, prob, uncertainty, precip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" group by uncertainty \"\"\"\n",
    "# sourcery skip: avoid-global-variables\n",
    "def grouped_acc_df(group: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"bin df by uncertainty\"\"\"\n",
    "    return pd.Series({'preds': group['preds'],\n",
    "                   'labels': group['labels'], \n",
    "                   'probs': group['probs'],\n",
    "                   'uncertainties': group['uncertainties'],\n",
    "                   'paths': group['paths'],\n",
    "                   'acc': (group['preds'] == group['labels']).mean(), \n",
    "                   'count': len(group)})\n",
    "\n",
    "df = pd.DataFrame(list(zip(preds, labels, probs, uncertainties, paths)), columns = ['preds', 'labels', 'probs', 'uncertainties', 'paths'])\n",
    "bins = np.linspace(0.0,1.0,11)\n",
    "df['binned'] = pd.cut(df['uncertainties'], bins)\n",
    "grouped_df = df.groupby('binned').apply(grouped_acc_df)\n",
    "uncertainty_acc_hist(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
